# Prompt Playground

A lightweight CLI-based prompt engineering playground to test how different prompts and generation parameters affect LLM output.

## âœ… Features

- Switch between prompt styles (direct, chain-of-thought, instructional)
- Adjust generation params: temperature, top_p, top_k, max_tokens
- Run against a locally hosted `vllm` model server

## ðŸš€ Getting Started

1. Ensure `vllm` is running at `http://localhost:8000`
2. Create a Python virtual environment:
